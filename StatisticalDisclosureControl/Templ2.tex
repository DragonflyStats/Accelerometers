\documentclass[]{article}

%opening
\title{}
\author{}

\begin{document}
	
%=========================================================%
%% 2.4. Sample Frequencies on Subsets: SUDA
%% 2.5. Calculating Cluster (Household) Risks

%% Section 2.6 Measuring the Global Risk

%%- 2 MEASURING THE DISCLOSURE RISK


%%-  For more information, see the help ﬁles of sdcMicro [Ternpl et al., 2013].
%========================================================= %

\subsubsection{2. Measuring the Disclosure Risk}
\begin{itemize}
%	\item Measuring risk in a micro dataset is a key task. Risk measurements are essential
%	to determine if the dataset is secure enough to be released. To assess disclosure
%	risk, one must make realistic assumptions about the information data users might
%	have at hand to match against the micro dataset, these assumptions are called
%	disclosure risk scenarios. 
%	
%	\item This goes hand in hand with the selection of categorical
%	key variables because the choice these identifying variables deﬁnes a speciﬁc dis-
%	closure risk scenario. 
%	\item The speciﬁc set of chosen key variables has direct influence
%	on the risk assessment because their distribution is a key input for the calculation
%	of both individual and global risk measures as it is now discussed.
	%------------------------------------------------------------------------------%
	\item Measuring risk in a micro dataset is a key task. Risk measurements are essential
	to determine if the dataset is secure enough to be released. To assess disclosure
	risk, one must make realistic assumptions about the information data users might
	have at hand to match against the micro dataset; these assumptions are called
	disclosure risk scenarios. This goes hand in hand with the selection of categorical
	key variables because the choice these identifying variables deﬁnes a speciﬁc disclo-
	sure risk scenario. 
	
	\item The speciﬁc set of chosen key variables has direct influence on
	the risk assessment because their distribution is a key input for the estimation of
	both individual and global risk measures as it is now discussed. For example, for a
	disclosure scenario for the European Union Structure of Earnings Statistics we can
	assume that information on company size, economic activity, age and earnings of
	employees are available in available data bases. Based on a speciﬁc disclosure risk
	scenario, it is necessary to deﬁne a set of key variables (i.e., identifying variables)
	that can be used as input for the risk evaluation procedure. Usually different sce-
	narios are considered. 
	
	\item For example, for the European Union Structure of Earnings
	Statistics a second scenario based on an additional key varibles is of interest to
	look at. e.g. occupation might be considered  well as an categorical key variable.
	The resulting risk might now be higher than for the previous scenario. It needs
	discussion with subject matter specialists which scenario is most realistic and an
	evaluation of different scenarios helps to get a broader picture about the disclosure
	risk in the data.
\end{itemize}

\newpage
%---------------------------------------------------------------------------------%
\subsection*{2.1. Population Frequencies and the Individual Risk Appoach}

\begin{itemize}
\item Typically, risk evaluation is based on the concept of uniqueness in the sample
and / or in the population. The focus is on individual units that possess rare com-
binations of selected key variables. The assumption is that units having rare
combinations of key variables can be more easily identiﬁed and thus have a higher
risk of re-identiﬁcation/disclosure. 
\item It is possible to cross-tabulate all identifying
variables and view their cast. Keys possessed by only very few individuals are
considered risky, especially if these observations also have small sampling weights.
This means that the expected number of individuals with these patterns is expected to be low in the population as well.
\end{itemize}
%% -- Page 6 / 31
%% 2 MEASURING THE DISCLOSURE RISK


\begin{itemize}
\item To assess whether a unit is at risk, a threshold approach is typically used. If the
risk of re—identiﬁcation for an individual is above a certain threshold value, the unit
is said to be at risk. To compute individual risks, it is necessary to estimate the
frequency of a given key pattern in the population. 

\item Let us deﬁne frequency counts
in a mathematical notation. Consider a random sample of size n drawn from a
ﬁnite population of size N. Let $\pi_j$, $j = 1, \ldots ,N$ be the (ﬁrst order) inclusion
probabilities — the probability that element $u_j$ of a population of the size N is
chosen in a sample of size n.
\end{itemize}
%----------------------------------------------------------------------------%
\begin{itemize}
\item All possible combinations of categories in the key variables (i.e., keys or patterns)
can be calculated by cross-tabulation of these variables. Let $f_i$, $i=1,2,\ldots,n$ be
the frequency counts obtained by cross-tabulation and let E be the frequency
counts of the population which belong to the same pattern. 
\item If $f_i=1$ applies,
the corresponding observation is unique in the sample given the key-variables. If
\item If $F_i = 1$, then the observation is unique in the population  well and automatically
unique or zero in the sample.
%----------------------------------------------------------------------------%
\item $F_i$ is usually not known, since, in statistics, information on samples is collected
to make inferences about populations.
\end{itemize}
\newpage
\begin{itemize}
\item In Table 1 a very simple data set is used to explain the calulation of sample
frequency counts and the (first rough) estimation of population frequency counts.
One can easily see that observation 1 and 8 are equal, given the key-variables Age
Class , Location, Sex and Education. Because the values of observations 1 and
8 are equal and therefore the sample frequency counts are $f_1=2$ and $f_8=2$.

\item Estimated population frequencies are obtained by summing up the sample weights
for equal observations. Population frequencies $\hat{F}_1$ and $\hat{F}_8$ can then be estimated
by summation over the corresponding sampling weights, $w_1$ and $w_8$. 
\item In summary,
two observations with the pattern (key) (1,2,5, 1) exist in the sample and 110
observations with this pattern (key) can be expected to exist in the population.
\end{itemize}
%----------------------------------------------------------------------------%
% % Graphic
% % Table 1: Example of sample and estimated population frequency counts.
% % TemplTable1.jpg

%----------------------------------------------------------------------------------------%
\begin{itemize}
	\item One can show, however, that these estimates almost always overestimate small
	population frequency counts .
	%see, e.g., Templ and l\leill<ll, 2010].
	
	\item A better approach is to use so-called super—population models, in which population frequency
	counts are modeled given certain distributions. 
	
	\item For example, the estimation pro-
	cedure of sample counts given the population counts can be modeled by assuming
	a negative binomial distribution and is implemented
	in \textbf{sdcMicro} in function \texttt{measurelisk()}
	and called by the
	\textbf{sdcMicroGUI}.
	
\end{itemize}

%%Page 7 / 31
%--------------------------------------------------------------------

%2 MEASURING THE DISCLOSURE RISK
Table 2: is-anonymity and l-diversity on a toy data set.
%==============================================%
\subsection*{2.2. k:-Anonymity}
Based on a set of key variables, one desired characteristic of a protected micro
dataset is often to achieve k-anonymity 
%[Samarati and Sweeney, 1998, Sarnarati,2001, Swwriey, 2002]. 
This means that each possible pattern of key variables con-
tains at least k units in the microdata. This is equal to $f_i \geq k, i=1,2,3,\ldots ,n$.  A
typical value is k = 3.
k—anonymity is typically achieved by recoding categorical key variables into fewer
categories and by suppressing speciﬁc values of key variables for some units;
% see Section 3.1 and 3.2.
%====================================================%
\subsection*{2.3. l-Diversity}
An extension of k-anonymity is l-diversity. Consider
a group of observations with the same pattern/keys in the key variables and let
the group fulfill k-anonymity. A data intruder can therefore by deﬁnition not
identify an individual within this group. If all observations have the same entries
in an additional sensitive variable. however (e.g., cancer in the variable medical
diagnosis), an attack will be successful if the attacker can identify at least one
individual of the group, as the attacker knows that this individual has cancer
with certainty. The distribution of the target-sensitive variable is referred to as
l-diversity.
%======================================================%
Table 2 considers a small example dataset that highlights the calculations of
l-diversity. It also points out the slight difference compared to lc-anonymity. The
ﬁrst two columns present the categorical key variables. The third column of the
data deﬁnes a variable containing sensitive information. 

Sample frequency counts $f_i$ appear in the fourth column. They equal 3 for the ﬁrst three observations; the
fourth observation is unique and frequency counts fi are 2 for the last two observa-
tions. Only the fourth observation violates 2-anonymity. Looking closer at the ﬁrst
three observations, we see that only two different values are present in the sensitive
variable. Thus the l-(distinct) diversity is just 2. 
%----------------------------------------------------------------------------%

For the last two observations,
2-anonymity is achieved, but the intruder still knows the exact information of the
sensitive variable. For these observations, the l-diversity measure is 1. indicating
that sensitive information can be disclosed, since the value of the sensitive variable
is = 62 for both of these observations.

Diversity in values of sensitive variables can be measured differently. We present
here the distinct diversity that counts how many different values exist within a
pattern. Additional methods such as entropy, recursive and multi-recursive are implemented in \textbf{sdcMicro}.

 implemented in sdcMicro.


%======================================================%
\subsection*{2.4. Sample Frequencies on Subsets: SUDA}
The \textbf{Special Uniques Detection Algorithm (SUDA)} is an often discussed method
to estimate the risk, but applications of this method can be rarely found. For
tlle sake of completeness this algorithm is implemented in sdcMicro (but not in
sdcMicroGUI) and explained in this document, but to evaluate the usefulness of
this method it needs more research. 

In the following the interested reader will
see that the SUDA approach is more than the sample frequency estimation shown
before. It consider also subsets of key variables. SUDA estimates disclosure risks
for each unit. SUDA2  is the computationally improved
version of SUDA. 
%% Cite:  [e.g., Manning et al., 2008]

It is a recursive algorithm to ﬁnd \textbf{\textit{Minimal Sample Uniques}}
(MSUs). SUDA2 generates all possible variable subsets of selected categorical key
variables and scans for unique patterns within subsets of these variables. The risk
of an observation primarily depends on two aspects:
%---------------------------------------------------------------------------------------%

\begin{itemize}
\item[(a)] The lower the number of variables needed to receive uniqueness, the higher
the risk (and the higher the SUDA score) of the corresponding observation.
\item[(b)] The larger the number of minimal sample uniqueness contained within an
observation, the higher the risk of this observation.
\end{itemize}
%---------------------------------------------------------------------------------------%

%_ m—l
%Item (a) is considered by calculating for each observation 1' by Z, — 1_[k_MSUm,-,,,1(m—
%la) ,i * I, ..., 11.


 In this formula, m corresponds to the depth, which is the max-
imum size of variable subsets of the key variables, \textit{MSUmin}$_i$, is the number of
MSUs of observation and i and n are the number of observations of the dataset.
%---------------------------------------------------------------------------------------%
\begin{itemize}
\item Since each observation is treated independently, a speciﬁc value 1,; belonging to a
speciﬁc pattern are summed up. This results in a common SUDA score for each
of the observations contained in this pattern; this summation is the contribution
mentioned in item 
\item The ﬁnal SUDA score is calculated by normalizing these SUDA scores by divid-
ing them by pl, with p being the number of key variables.
\end{itemize}

%------------------------------------------------------------------%
\newpage
\subsection*{Data Intrusion Score}
\begin{itemize} 
\item To receive the so-called
\textbf{Data Intrusion Simulation (DIS) score}, loosely speaking, an iterative algorithm
based on sampling of the data and matching of subsets of the sampled data with
the original data is applied. 
\item This algorithm calculates the probabilities of correct
matches given unique matches. It is, however, out of scope to precisely describe
this algorithm here. %reference Fllliot [2000] for details. 
\item The DIS SUDA score is calculated from the SUDA and DIS scores, and is available in sdcMicro as \texttt{disScore()}.
\item Note that this method does not consider population frequencies in general, but
does consider sample frequencies on subsets. The DIS SUDA scores approximate
uniqueness by simulation based on the sample information population, but to our
knowledge, they generally do not consider sampling weights, and biased estimates
may therefore result.

\item In Table 3, we use the same test dataset as in Section 2.1. Sample frequency
counts $f_i$ as well as the SUDA and DIS SUDA scores have been calculated. The
SUDA scores have the largest value for observation 4 and 6 since subsets of key
variables of these observation are also unique, while for observations 1 — 3, 5 and
8, less subsets are unique.
\end{itemize}
%%--- Page 9 / 31
%--------------------------------------------------------------------------------------------------------%
% 2 MEASURING THE DISCLOSURE RISK
Table 3: Example of SUDA scores (scores) and DIS SUDA scores (disScores).

%%% TemplTable3.jpg

%----------------------------------------------------------------------------------------------%
In sdcMicro (function \texttt{suda2()}) additional output, such as the contribution
percentages of each variable to the score, are available. The contribution to the
SUDA score is calculated by assessing how often a category of a key variable
contributes to the score.
%======================================================%
\newpage
\subsection*{2.5. Calculating Cluster (Household) Risks}
\begin{itemize}
\item Micro datasets often contain hierarchical cluster structures; an example is social
surveys, when individuals are clustered in households. 
\item The risk of re-identifying
an individual within a household may also affect the probability of disclosure of
other members in the same household. Thus, the household or cluster-structure of
the data must be taken into account when calculating risks.

\item It is commonly assumed that the risk of re-identﬁcation of a household is the risk
that at least one member of the household can be disclosed. Thus this probability
can be simply estimated from individual risks as 1 minus the probability that no
member of the household can be identﬁed. 
\item Thus, if we consider a single household
with three persons that have individual risks of re-identiﬁcation of 0.1, 0.05 and
0.01, respectively, the risk-measure for the entire household will be calculated as
1-(0.1+0.05+0.01). This is also the implementation strategy from \textbf{sdcMicro}.
\end{itemize}
\newpage
%===================================================================================================%
\subsection*{2.6. Measuring the Global Risk}

Sections 2.1 through 2.5 discuss the theory of individual risks and the extension
of this approach to clusters such as households. ln many applications, however,
estimating a measure of global risk is preferred. Any global risk measure is result
in one single number that can be used to assess the risk of an entire micro dataset.
The following global risk measures are available in sdcMicroGUI, except the last
one presented in Section 2.7.2 that is computationally expensive is only made
available in sdcMicro.
%=================================================================%
\subsection*{2.6.1. Measuring the global risk using individual risks}
Two approaches can be used to determine the global risk for a dataset using
individual risks:
\begin{description}
\item[Benchmark:] This approach counts the number of observations that can be con-
sidered risky and also have higher risk as the main part of the data. For
example, we consider units with individual risks being both $\geq 0.1$ and twice
as large as the median of all individual risks + 2 times the median abso-
lute deviation (MAD) of all unit risks. This statistics in also shown in the
\textbf{sdcMicroGUI}.

\item[Global risk:] The sum of the individual risks in the dataset gives the expected
number of re—identiﬁcations

%%%%CITE  [see Hundepool et al., 2008].
\end{description}
The benchmark approach indicates whether the distribution of individual risk
occurrences contains extreme values; it is a relative measure that depends on the
distribution of individual risks. It is not valid to conclude that observations with
higher risk as this benchmark are of very high risk; it evaluates whether some
unit risks behave differently compared to most of the other individual risks. The
global risk approach is based on an absolute measure of risk. 

Following is the print
output of the corresponding function from sdclvlicro, which shows both measures
(see the example in the manual of sdcMicro [Tcinpl ct al., 2013]):

%% CODE OUTPUT HERE

The global risk measurement taking into account this hierarchical structure if a
variable expressing it is defined.
%%-----------------------------------------------------------------------------------------------------%
%\subsection*{2.6.2. Measuring the global risk using log-linear models}
%Sample frequencies, considered for each of [W patterns 'm, fm ,m : 1, ..., 1% can
%be modeled by a Poisson distribution. In this case, global risk can be deﬁned as
%the following [see also Skinner and Holmes, 1998]:
%
%%% EQUATION
%
%For simplicity, the (ﬁrst order) inclusion probabilities are assumed to be equal,
%71',” = 7T ,m = 1. ..., ZVI. T1 can be estimated by log-linear models that include both
%the primary effects and possible interactions. This model is defined as:
%
%%% EQUATION
%
%To estimate the ,a,,,’s, the regression coefficients /2’ have to be estimated using,
%for example, iterative proportional ﬁtting. The quality of this risk measurement
%approach depends o11 the number of different keys that result from cross-tabulating
%all key variables. If the cross—tabulated key variables are sparse in terms of how
%many observations have the same patterns, predicted values might be of low quality.
%It must also be considered that if the model for prediction is weak, the quality of
%the prediction of the frequency counts is also weak. Thus, the risk measurement
%%% Page 11 / 31
%%% 2 MEASURING THE DISCLOSURE RISK
%with log-linear models may lead to acceptable estimates of global risk only if not
%too many key variables are selected and if good predictors are available in the
%dataset.
%%----------------------------------------------------------------------------------------%
%In sdcMicro, global risk measurement using log-linear models can be completed
%with function \texttt{LLmodGlobalRisk()}. This function is experimental and needs further
%testing, however. It should be used only by expert users.

\subsection*{2.7. Measuring Risk for Continuous Key Variables}
The concepts of uniqueness and k—anonymity cannot be directly applied to con-
tinuous key variables because almost every unit in the dataset will be identiﬁed as
unique. As a result, this approach will fail. The following sections present methods
to measure risk for continuous key variables.


\subsection*{2.7.1. Distance-based record linkage}
If detailed information about a value of a continuous variable is available, i.e. the
risk comes from the fact that multiple datasets can be available to the attacker,
one of which contains identiﬁers like income, for example, attackers may be able
to identify and eventually obtain further information about an individual. Thus,
an intruder may identify statistical units by applying, for example, linking or
matching algorithms. The anonymization of continuous key variables should avoid
the possibility of successfully merging the underlying microdata with other external
data sources.
%-------------------------------------------------------------------------------------------------%
We assume that an intruder has information about a statistical unit included
in the microdata; the intruder’s information overlaps on some variables with the
information in the data. In simpler terms, We assume that the intruderls informa-
tion can be merged with niicrodata that should be secured. In addition, we also
assume that the intruder is sure that the link to the data is correct, except for
micro-aggregated data (see Section 3.4). Domingo-Fcrrcr and Torra [2001] showed
that these methods outperform probabilistic methods.

Mateo-Sanz et al. [2004] introduced distance-based record linkage and interval
disclosure. In the ﬁrst approach, they look for the nearest neighbor from each
observation of the masked data value to the original data points. Then they mark
those units for which the nearest neighbor is the corresponding original value.

In the second approach, they check if the original value falls within an interval
centered on the masked value. Then they calculate the length of the intervals
based on the standard deviation of the variable under consideration (see Figure 2,
upper left graphic; the boxes expresses the intervals).
%--------------------------------------------------------------------------------------------------------------%
\subsection*{2.7.2. Special treatment of outliers when calculating disclosure risks}

It is worth to show alternatives to the previous distance-based risk measure. Such
alternatives took either distances between every observation into account or are
based on covariance estimation (as shown here). Thus, they are computationlly
more intensive, which is also the reason why they are not available in sdcMicroGUI
but only in sdcMicro for experienced users.

Almost all datasets used in ofﬁcial statistics contain units whose values in at least
one variable are quite different from the general observations. As a result, these
variables are very asymmetrically distributed. Examples of such outliers might
%%Page 12 / 31

%======================================================================================%
%% 2 MEASURING THE DISCLOSURE RISK
be enterprises with a very high value for turnover or persons with extremely high
income. In addition, multivariate outliers exist [see \textit{Templ and Meindl, 2008a}].

\begin{itemize}
	\item Unfortunately, intruders may want to disclose a large enterprise or an enterprise
	with speciﬁc characteristics. Since enterprises are often sampled with certainty or
	have a sampling weight close to 1, intruders can often be very conﬁdent that the
	enterprise they want to disclose has been sampled. 
	\item In contrast, an intruder may
	not be as interested to disclose statistical units that exhibit the same behavior as
	most other observations. For these reasons, it is good practice to deﬁne measures
	of disclosure risk that take the outlyingness of an observation into account. 
	% For details, see Templ and l\Iein<ll [2U08a]. 
	\item Outliers should be much more perturbed than
	non—outliers because these units are easier to re-identify even when the distance
	from the masked observation to its original observation is relatively large.
\end{itemize}

This method for risk estimation (called RMDID2 in Figure 2) is also included
in the \textbf{sdcMicro} package. 
It works as described in \textit{Templ and Meindl [2008a]} and
is listed as follows:
\begin{itemize}
	\item[(1.)] Robust mahalanobis distances (RMD) [see, for example Maronna ct al., 2006]
	are estimated between observations (continuous variables) to obtain a robust,
	multivariate distance for each unit.
	\item[(2.)] Intervals are estimated for each observation around every data point of the
	original data points. The length of the intervals depends on squared distances
	calculated in step 1 and an additional scale parameter. The higher the RMD
	of an observation, the larger the corresponding intervals.
	\item[(3.)] Check whether the corresponding masked values of a unit fall into the in-
	tervals around the original values. If the masked value lies within such an
	interval, the entire observation is considered unsafe. We obtain a vector in-
	dicating which observations are safe or which are not. For all unsafe units,
	at least m other observations from the masked data should be very close.
\end{itemize}
%--------------------------------------------------------------------------------------------------------%
Close is quantiﬁed by specifying a parameter for the length of the intervals
around this observation using Euclidean distances. If more than m points lie
within these small intervals, we can conclude that the observation is safe.
%--------------------------------------------------------------------------------------------------------%
Figure 2 depicts the idea of weighting disclosure risk intervals. For simple meth-
ods (top left and right graphics), the rectangular regions around each value are
the same size for each observation. Our proposed methods take the RMDs of
each observation into account. The difference between the bottom right and left
graphics is that, for method RMDID2, rectangular regions are calculated around
each masked variable as well. If an observation of the masked variable falls into an
interval around the original value, check whether this observation has close neigh-
bors. If the values of at least m other masked observations can be found inside a
second interval around this masked observation, these observations are considered
safe.


These methods are also implemented and available in sdcMicro as \texttt{dRisk()} and
\texttt{dRiskRMD()}. The former is automatically applied to objects of class sdcMi-
cr'0Obj, while the latter has to be speciﬁed explicitly and can currently not be
applied using the graphical user interface.
\newpage
Page 13 / 31

\textit{Figure 2: Original and corresponding masked observations (perturbed by adding
	additive noise). In the bottom right graphic, small additional regions are
	plotted around the masked values for RMDID2 procedures. The larger
	the intervals the more the observations is an outlier for the latter two
	methods.}
%====================================================%
\newpage
\end{document}

%Page 8 / 31


%% OLD MATERIAL BELOW

%=============================================================================================================%
%% - 1 CONCEPTS

What is the legal situation regarding data privacy? Laws on data privacy vary
between countries; some have quite restrictive laws, some don’t, and laws often
differ for different kinds of data (e.g., business statistics, labor force statistics,
social statistics, and medical data) 